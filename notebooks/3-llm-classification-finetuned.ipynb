{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Security - Prompt Injection\n",
    "## Part 3 - Classification Using a Fine-tuned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we load the raw dataset and fine-tune a pre-trained large language model to classify malicious prompts.\n",
    "> **INPUT:** the raw dataset loaded from Hugging Face library. <br>\n",
    "> **OUTPUT:** the performance analysis of fine-tuned LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LOADING DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to use a pre-trained LLM and fine-tune it using the training dataset, we need to load both training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data set location and file name\n",
    "data_file_path = \"../data/raw/\"\n",
    "data_file_name_train = \"train-00000-of-00001-9564e8b05b4757ab\"\n",
    "data_file_name_test = \"test-00000-of-00001-701d16158af87368\"\n",
    "data_file_ext = \".parquet\"\n",
    "\n",
    "# Loading data set into a pandas DataFrame\n",
    "data_train = pd.read_parquet(data_file_path + data_file_name_train + data_file_ext)\n",
    "data_test = pd.read_parquet(data_file_path + data_file_name_test + data_file_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"text\" column into \"prompt\"\n",
    "data_train.rename(columns={\"text\":\"prompt\"}, inplace=True)\n",
    "data_test.rename(columns={\"text\":\"prompt\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already explored the dataset in the previous notebooks, so we will directly proceed to the fine-tuning phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MODEL FINE-TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we aim at using a pre-trained LLM and fine-tuning it on the classification task.\n",
    "\n",
    "In the previous experiment, we used the pre-trained XLM-RoBERTa, the multilingual version of RoBERTa (Robustly optimized BERT approach), the enhanced version of BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "To explore the effect of model fine-tuning, we fine-tune XLM-RoBERTa on our training dataset and then re-evaluate its performance in correctly predicting prompt injections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model, its tokenizer, and torch library\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function to take a batch of data and tokenize the prompts\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch['prompt'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize prompts in both training and testing datasets\n",
    "prompts_train_tokenized = tokenize_batch(data_train.to_dict(orient='list'))\n",
    "prompts_test_tokenized = tokenize_batch(data_test.to_dict(orient='list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset Class to work with PyTorch's DataLoader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(prompts_train_tokenized, data_train['label'])\n",
    "test_dataset = CustomDataset(prompts_test_tokenized, data_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-large', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the model, we get this message indicating that some weights are not initialized. This is normal since the model is pre-trained but not fine-tuned for the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TrainingArguments to handle the various training configurations\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../output\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"../output/logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to track the model performance\n",
    "results_df = pd.DataFrame(columns=[\"epoch\",\"accuracy\",\"precision\",\"recall\",\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# A utility function for model evaluation during fine-tuning\n",
    "def evaluate_model(trainer, epoch):\n",
    "    \n",
    "    # Extract predictions and labels\n",
    "    predictions, labels = trainer.predictions.argmax(axis=1), trainer.label_ids\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate precision, recall, and f1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\", zero_division=1)\n",
    "    \n",
    "    # Append current metrics to results\n",
    "    global results_df\n",
    "    results_df.loc[len(results_df)] = [epoch, accuracy, precision, recall, f1]\n",
    "        \n",
    "    # Return\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Trainer class\n",
    "from transformers import Trainer\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=lambda p: evaluate_model(p, trainer.state.epoch),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model\n",
    "final_results = trainer.evaluate()\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning is finished now, we will save the model to a local path.\n",
    "\n",
    "This step is preferred so we can reload it later for evaluation and inference without the need to repeat the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local model path\n",
    "models_path = \"../models/xlm_roberta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer for a later use \n",
    "model.save_pretrained(models_path)\n",
    "tokenizer.save_pretrained(models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RESULT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the locally-stored fine-tuned model\n",
    "loaded_model = XLMRobertaForSequenceClassification.from_pretrained(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
