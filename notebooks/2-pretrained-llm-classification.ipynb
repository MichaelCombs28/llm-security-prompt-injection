{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Security - Prompt Injection\n",
    "## Part 2 - Classification Using a Pre-trained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we load the raw dataset and use a pre-trained large language model to to spot malicious prompts.\n",
    "> **INPUT:** the raw dataset loaded from Hugging Face library. <br>\n",
    "> **OUTPUT:** the performance analysis of considered LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LOADING DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a pre-trained model without fine tuning, there is no need to load the training data set.\n",
    "\n",
    "However, we are loading it anyway to validate the performance on the whole dataset, including both training and testing samples, to obtain an wider evaluation of the model performance.\n",
    "\n",
    "Surely, only testing dataset performance will be considered for the comparison with other classification approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data set location and file name\n",
    "data_file_path = \"../data/raw/\"\n",
    "data_file_name_train = \"train-00000-of-00001-9564e8b05b4757ab\"\n",
    "data_file_name_test = \"test-00000-of-00001-701d16158af87368\"\n",
    "data_file_ext = \".parquet\"\n",
    "\n",
    "# Loading data set into a pandas DataFrame\n",
    "data_train = pd.read_parquet(data_file_path + data_file_name_train + data_file_ext)\n",
    "data_test = pd.read_parquet(data_file_path + data_file_name_test + data_file_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"text\" column into \"prompt\"\n",
    "data_train.rename(columns={\"text\":\"prompt\"}, inplace=True)\n",
    "data_test.rename(columns={\"text\":\"prompt\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already explored the dataset in the previous notebook, so we will directly proceed to the inference phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MODEL PREDICTION (mBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we aim at using a pre-trained LLM without fine-tuning the model on the downstream task.\n",
    "\n",
    "Since the prompts in the dataset combine different languages, we should rely on a multilingual model that is trained on text of the languages indicated in the existing prompts.\n",
    "\n",
    "For this purpose, we will use BERT multilingual base model (uncased) from Hugging Face library. This model is pre-trained on the top 102 languages with the largest Wikipedia using a masked language modeling (MLM) objective.\n",
    "\n",
    "Givin the original task of mBERT is to predict masked words, we have to tailor this goal towards predicting whether a prompt is an injection or not. For this reason, we will utilize the \"zero-shot-classification\" task provided by Hugging Face pipeline to achieve this goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline functionality from the Hugging Face transformer's library\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "# Load the fill mask classification pipeline with mBERT\n",
    "classifier = pipeline(task=\"zero-shot-classification\", model=\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify the prompt\n",
    "def classify_prompt(prompt):\n",
    "    # List of candidate labels (in this case, indicating whether the text is an injection or not)\n",
    "    candidate_labels = [\"Injection\", \"Normal\"]\n",
    "    \n",
    "    # Perform zero-shot classification\n",
    "    output = classifier(prompt, candidate_labels)\n",
    "    \n",
    "    # Return the result\n",
    "    return 1 if output['labels'][0] == \"Injection\" else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classifier on both training and testing datasets\n",
    "data_train[\"predicted_label\"] = data_train[\"prompt\"].apply(classify_prompt)\n",
    "data_test[\"predicted_label\"] = data_test[\"prompt\"].apply(classify_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RESULT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import performance metrics libraries\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Prepare a variable to keep track of the models' performance\n",
    "results = pd.DataFrame(columns=[\"accuracy\", \"precision\", \"recall\", \"f1 score\"])\n",
    "\n",
    "for data in [(\"Training Data\", data_train),(\"Testing Data\", data_test)]:\n",
    "\n",
    "    # Initialize actual and predicted labels\n",
    "    y_test = data[1][\"label\"]\n",
    "    y_predict = data[1][\"predicted_label\"]\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    precision = precision_score(y_test, y_predict)\n",
    "    recall = recall_score(y_test, y_predict)\n",
    "    f1 = f1_score(y_test, y_predict)\n",
    "\n",
    "    # Store performance metrics\n",
    "    results.loc[data[0]] = [accuracy, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Data</th>\n",
       "      <td>0.391941</td>\n",
       "      <td>0.375242</td>\n",
       "      <td>0.955665</td>\n",
       "      <td>0.538889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Testing Data</th>\n",
       "      <td>0.491379</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.642424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  precision    recall  f1 score\n",
       "Training Data  0.391941   0.375242  0.955665  0.538889\n",
       "Testing Data   0.491379   0.504762  0.883333  0.642424"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check obtained results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above we can notice the following points:\n",
    "\n",
    "**Training Accuracy:**\n",
    "- The training accuracy is relatively low (39.19%), which might indicate that the model is not fitting the training data well.\n",
    "\n",
    "**Precision and Recall:**\n",
    "- The precision on training data (37.52%) suggests that when the model predicts an injection, it is correct around 37.52% of the time.\n",
    "- The recall on training data (95.57%) indicates that the model captures a high percentage of actual injections. However, the low precision suggests that there are many false positives.\n",
    "\n",
    "**Testing Accuracy:**\n",
    "- The testing accuracy (49.14%) is slightly better than the training accuracy, but still not very high.\n",
    "\n",
    "**Precision and Recall on Testing Data:**\n",
    "- The precision on testing data (50.48%) is slightly higher than in training, indicating that when the model predicts an injection, it is correct around 50.48% of the time.\n",
    "- The recall on testing data (88.33%) suggests that the model captures a high percentage of actual injections in the testing set, but there might be cases it misses.\n",
    "\n",
    "**F1 Score:**\n",
    "- The F1 score on both training (53.89%) and testing data (64.24%) is a harmonic mean of precision and recall. It's a balanced metric, and in this case, the testing F1 score is higher, which is a positive sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
