{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Security - Prompt Injection\n",
    "## Part 3 - Classification Using a Fine-tuned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we load the raw dataset and fine-tune a pre-trained large language model to classify malicious prompts.\n",
    "> **INPUT:** the raw dataset loaded from Hugging Face library. <br>\n",
    "> **OUTPUT:** the performance analysis of fine-tuned LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LOADING DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to use a pre-trained LLM and fine-tune it using the training dataset, we need to load both training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data set location and file name\n",
    "data_file_path = \"../data/raw/\"\n",
    "data_file_name_train = \"train-00000-of-00001-9564e8b05b4757ab\"\n",
    "data_file_name_test = \"test-00000-of-00001-701d16158af87368\"\n",
    "data_file_ext = \".parquet\"\n",
    "\n",
    "# Loading data set into a pandas DataFrame\n",
    "data_train = pd.read_parquet(data_file_path + data_file_name_train + data_file_ext)\n",
    "data_test = pd.read_parquet(data_file_path + data_file_name_test + data_file_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"text\" column into \"prompt\"\n",
    "data_train.rename(columns={\"text\":\"prompt\"}, inplace=True)\n",
    "data_test.rename(columns={\"text\":\"prompt\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already explored the dataset in the previous notebooks, so we will directly proceed to the fine-tuning phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MODEL FINE-TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we aim at using a pre-trained LLM and fine-tuning it on the classification task.\n",
    "\n",
    "In the previous experiment, we used the pre-trained XLM-RoBERTa, the multilingual version of RoBERTa (Robustly optimized BERT approach), the enhanced version of BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "To explore the effect of model fine-tuning, we fine-tune XLM-RoBERTa on our training dataset and then re-evaluate its performance in correctly predicting prompt injections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model, its tokenizer, and torch library\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function to take a batch of data and tokenize the prompts\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch['prompt'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize prompts in both training and testing datasets\n",
    "prompts_train_tokenized = tokenize_batch(data_train.to_dict(orient='list'))\n",
    "prompts_test_tokenized = tokenize_batch(data_test.to_dict(orient='list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset Class to work with PyTorch's DataLoader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(prompts_train_tokenized, data_train['label'])\n",
    "test_dataset = CustomDataset(prompts_test_tokenized, data_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-large', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the model, we get this message indicating that some weights are not initialized. This is normal since the model is pre-trained but not fine-tuned for the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TrainingArguments to handle the various training configurations\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../output\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"../output/logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to track the model performance\n",
    "results_df = pd.DataFrame(columns=[\"epoch\",\"accuracy\",\"precision\",\"recall\",\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# A utility function for model evaluation during fine-tuning\n",
    "def evaluate_model(trainer, epoch):\n",
    "    \n",
    "    # Extract predictions and labels\n",
    "    predictions, labels = trainer.predictions.argmax(axis=1), trainer.label_ids\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate precision, recall, and f1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\", zero_division=1)\n",
    "    \n",
    "    # Append current metrics to results\n",
    "    global results_df\n",
    "    results_df.loc[len(results_df)] = [epoch, accuracy, precision, recall, f1]\n",
    "        \n",
    "    # Return\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Trainer class\n",
    "from transformers import Trainer\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=lambda p: evaluate_model(p, trainer.state.epoch),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd261fe2d9b74a2388ff08edb8f0f0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a156a61f6340f69e8f27b4f5b222f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7632972598075867, 'eval_accuracy': 0.5172413793103449, 'eval_precision': 0.7586206896551724, 'eval_recall': 0.5172413793103449, 'eval_f1': 0.3864942528735632, 'eval_runtime': 29.2446, 'eval_samples_per_second': 3.967, 'eval_steps_per_second': 0.513, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40e509dd8e34d64b9a449486b9e9155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26779186725616455, 'eval_accuracy': 0.9396551724137931, 'eval_precision': 0.946360153256705, 'eval_recall': 0.9396551724137931, 'eval_f1': 0.9395609327038719, 'eval_runtime': 33.683, 'eval_samples_per_second': 3.444, 'eval_steps_per_second': 0.445, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835778674f45443aa37d189375889c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1886318027973175, 'eval_accuracy': 0.9655172413793104, 'eval_precision': 0.967816091954023, 'eval_recall': 0.9655172413793104, 'eval_f1': 0.9655172413793104, 'eval_runtime': 26.9663, 'eval_samples_per_second': 4.302, 'eval_steps_per_second': 0.556, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd2e828a0094c9fbcb933b610bc3b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1582818180322647, 'eval_accuracy': 0.9741379310344828, 'eval_precision': 0.9754529514903565, 'eval_recall': 0.9741379310344828, 'eval_f1': 0.9741436973820781, 'eval_runtime': 27.0063, 'eval_samples_per_second': 4.295, 'eval_steps_per_second': 0.555, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f3311ce83d45f88e5bdb2d57d35783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26131242513656616, 'eval_accuracy': 0.9655172413793104, 'eval_precision': 0.967816091954023, 'eval_recall': 0.9655172413793104, 'eval_f1': 0.9655172413793104, 'eval_runtime': 33.9591, 'eval_samples_per_second': 3.416, 'eval_steps_per_second': 0.442, 'epoch': 5.0}\n",
      "{'train_runtime': 11414.4584, 'train_samples_per_second': 0.239, 'train_steps_per_second': 0.03, 'train_loss': 0.2081522844839787, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=345, training_loss=0.2081522844839787, metrics={'train_runtime': 11414.4584, 'train_samples_per_second': 0.239, 'train_steps_per_second': 0.03, 'train_loss': 0.2081522844839787, 'epoch': 5.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.386494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.946360</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.939561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.967816</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.965517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.975453</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.974144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.967816</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.965517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.967816</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.965517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  accuracy  precision    recall        f1\n",
       "0    1.0  0.517241   0.758621  0.517241  0.386494\n",
       "1    2.0  0.939655   0.946360  0.939655  0.939561\n",
       "2    3.0  0.965517   0.967816  0.965517  0.965517\n",
       "3    4.0  0.974138   0.975453  0.974138  0.974144\n",
       "4    5.0  0.965517   0.967816  0.965517  0.965517\n",
       "5    5.0  0.965517   0.967816  0.965517  0.965517"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print detailed results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1cdc49002414a9f9c976d88730c2883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26131242513656616, 'eval_accuracy': 0.9655172413793104, 'eval_precision': 0.967816091954023, 'eval_recall': 0.9655172413793104, 'eval_f1': 0.9655172413793104, 'eval_runtime': 28.3387, 'eval_samples_per_second': 4.093, 'eval_steps_per_second': 0.529, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the final model\n",
    "final_results = trainer.evaluate()\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning is finished now, we will save the model to a local path.\n",
    "\n",
    "This step is preferred so we can reload it later for evaluation and inference without the need to repeat the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local model path\n",
    "models_path = \"../models/xlm_roberta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/xlm_roberta\\\\tokenizer_config.json',\n",
       " '../models/xlm_roberta\\\\special_tokens_map.json',\n",
       " '../models/xlm_roberta\\\\sentencepiece.bpe.model',\n",
       " '../models/xlm_roberta\\\\added_tokens.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer for a later use \n",
    "model.save_pretrained(models_path)\n",
    "tokenizer.save_pretrained(models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RESULT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the locally-stored fine-tuned model\n",
    "loaded_model = XLMRobertaForSequenceClassification.from_pretrained(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
