# llm-security-prompt-injection
This project investigates the security of large language models by performing binary classification of a set of input prompts to discover malicious prompts. Several approaches have been analyzed using classical ML algorithms, a trained LLM model, and a fine-tuned LLM model.

Important: this project is still under development, and further commits are expected regularly until the project is done. The readme file will ultimately updated when all major implementations are finalized.
