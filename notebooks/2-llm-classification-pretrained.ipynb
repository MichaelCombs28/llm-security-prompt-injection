{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Security - Prompt Injection\n",
    "## Part 2 - Classification Using a Pre-trained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we load the raw dataset and use a pre-trained large language model to to spot malicious prompts.\n",
    "> **INPUT:** the raw dataset loaded from Hugging Face library. <br>\n",
    "> **OUTPUT:** the performance analysis of considered LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LOADING DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a pre-trained model without fine tuning, there is no need to load the training data set.\n",
    "\n",
    "However, we are loading it anyway to validate the performance on the whole dataset, including both training and testing samples, to obtain an wider evaluation of the model performance.\n",
    "\n",
    "Surely, only testing dataset performance will be considered for the comparison with other classification approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data set location and file name\n",
    "data_file_path = \"../data/raw/\"\n",
    "data_file_name_train = \"train-00000-of-00001-9564e8b05b4757ab\"\n",
    "data_file_name_test = \"test-00000-of-00001-701d16158af87368\"\n",
    "data_file_ext = \".parquet\"\n",
    "\n",
    "# Loading data set into a pandas DataFrame\n",
    "data_train = pd.read_parquet(data_file_path + data_file_name_train + data_file_ext)\n",
    "data_test = pd.read_parquet(data_file_path + data_file_name_test + data_file_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"text\" column into \"prompt\"\n",
    "data_train.rename(columns={\"text\":\"prompt\"}, inplace=True)\n",
    "data_test.rename(columns={\"text\":\"prompt\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already explored the dataset in the previous notebook, so we will directly proceed to the inference phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MODEL PREDICTION (mBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we aim at using a pre-trained LLM without fine-tuning the model on the downstream task.\n",
    "\n",
    "Since the prompts in the dataset combine different languages, we should rely on a multilingual model that is trained on text of the languages indicated in the existing prompts.\n",
    "\n",
    "For this purpose, RoBERTa (Robustly optimized BERT approach), the enhanced version of BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "RoBERTa is a transformer-based neural network model developed by Facebook AI, designed for natural language understanding tasks. Since out dataset is multilingual, wi will use XLM-RoBERTa variant, with is a multilingual version of RoBERTa pre-trained on data containing 100 languages.\n",
    "\n",
    "Givin the original task of XLM-RoBERTa is to predict masked words, we have to tailor this goal towards predicting whether a prompt is an injection or not. For this reason, we will utilize the \"zero-shot-classification\" task provided by Hugging Face pipeline to achieve this goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline functionality from the Hugging Face transformer's library\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "# Load the fill mask classification pipeline with mBERT\n",
    "classifier = pipeline(task=\"zero-shot-classification\", model=\"xlm-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify the prompt\n",
    "def classify_prompt(prompt):\n",
    "    # List of candidate labels (in this case, indicating whether the text is an injection or not)\n",
    "    candidate_labels = [\"Injection\", \"Normal\"]\n",
    "    \n",
    "    # Perform zero-shot classification\n",
    "    output = classifier(prompt, candidate_labels)\n",
    "    \n",
    "    # Return the results\n",
    "    return 1 if output['labels'][0] == \"Injection\" else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classifier on both training and testing datasets\n",
    "data_train[\"predicted_label\"] = data_train[\"prompt\"].apply(classify_prompt)\n",
    "data_test[\"predicted_label\"] = data_test[\"prompt\"].apply(classify_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RESULT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import performance metrics libraries\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Prepare a variable to keep track of the models' performance\n",
    "results = pd.DataFrame(columns=[\"accuracy\", \"precision\", \"recall\", \"f1 score\"])\n",
    "\n",
    "for data in [(\"Training Data\", data_train),(\"Testing Data\", data_test)]:\n",
    "\n",
    "    # Initialize actual and predicted labels\n",
    "    y_test = data[1][\"label\"]\n",
    "    y_predict = data[1][\"predicted_label\"]\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    precision = precision_score(y_test, y_predict)\n",
    "    recall = recall_score(y_test, y_predict)\n",
    "    f1 = f1_score(y_test, y_predict)\n",
    "\n",
    "    # Store performance metrics\n",
    "    results.loc[data[0]] = [accuracy, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Data</th>\n",
       "      <td>0.558608</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.561576</td>\n",
       "      <td>0.486141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Testing Data</th>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.559322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  precision    recall  f1 score\n",
       "Training Data  0.558608   0.428571  0.561576  0.486141\n",
       "Testing Data   0.551724   0.568966  0.550000  0.559322"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check obtained results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above we can notice the following points:\n",
    "\n",
    "- Accuracy of both training and testing is relatively low, which indicates that the model was not able to capture the characteristics of injections in the dataset.\n",
    "- The model's performance is relatively consistent between training and testing data, indicating reasonable generalization.\n",
    "- F1 score suggests a balanced performance, but efforts to enhance both precision and recall are warranted.\n",
    "- The model's performance could benefit from fine-tuning, especially given the nature of the classification task involving injection prompts.\n",
    "- The low performance is quite expected given XLM-RoBERTa model is mostly intended to be fine-tuned on the downstream task before being used for classification.\n",
    "\n",
    "In summary, while the model shows moderate performance, there is room for enhancement, and fine-tuning on task-specific data is likely to yield improvements in precision, recall, and overall classification effectiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
